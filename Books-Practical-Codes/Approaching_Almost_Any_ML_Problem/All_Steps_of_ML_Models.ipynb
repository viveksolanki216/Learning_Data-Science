{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Author:** Vivek Singh Solanki\n",
    "**Book:** Approaching Almost Any Machine Learning Problem.\n",
    "**Date:**  04-03-2023\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning\n",
    "## Supervised v/s Unsupervised ML\n",
    "  * **Supervised ML:** Learning models where predicting single or multiple variables.\n",
    "    * <b>Classification v/s Regression:</b> Predicting a category v/s a numeric value.\n",
    "  * <b>Un-supervised ML:</b> No target variable, and we need to find patterns or group them.\n",
    "    * __Clustring:__ Instances with no target variable, can be grouped/divided into clusters and helps us to identity patterns. <br> i.e. Credit Card Transactions, where lots of data comes every second, and it gets very difficult to mark them fraud/genuine trx using humans. We can use clustering to group those transactions and find patterns and try to figure out abnormal behaviours."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross-Validation\n",
    "* It's a step in process of building ML model for a problem that ensures that the ML model *fits the data accurately* and make sure that it does not *overfit*.\n",
    "* Helps us choose best model from a set of candidate hypotheses.\n",
    "* Helps us find the estimate of performance of the model for production-live version, by mimicking the production live enviornment using the existing data.\n",
    "\n",
    "##### Overfitting:\n",
    "* When a model learns the training data well but fails to generalize unseen samples/ test data. High performance on train-set but very low on test-set.\n",
    "* A hypothesis overfits the training examples, if some other hypothesis that fits the training data less well by performs better on unseen data or over the entire distribution of instances.\n",
    "* High Variance, Low Bias.\n",
    "\n",
    "*Occam's Razor* stats that one should not try to complicate things that can be solved in much simpler manner.\n",
    "**Arises when;**\n",
    "   * We use a complex hypothesis/model for simpler ones.\n",
    "   * When there is noise in the data\n",
    "   * \\# of training examples is too small to produce a representative sample of the true target function.\n",
    "      *\n",
    "\n",
    "##### How Cross-Validation is done?\n",
    "It's simple divide data into two sets, one to train the models and another to test (hold-out/validation/test set) the performance. Though there many methods to cross-validate the models;\n",
    " * __Hold-out:__ above discussed. When data is large and it's expensive to train model several times. time-series data. etc\n",
    " * __k-fold:__ Divide the data randomly into k sets exclusive of each other, use k-1 sets for training and remained one set to test and repeat this k times.\n",
    " * __stratified k-fold:__ when classes are highly imbalanced, dividing randomly could lead no or very less samples for small class, you need to ensure the proportion of that in each of the k sets.\n",
    " * __Leave one out: __\n",
    " * __Group k-fold:__\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation Metrics\n",
    "  * Classifications Metrics:\n",
    "    * Accuracy, Precision, Recall, F1-Score, TPR, FPR, Area under the ROC curve\n",
    "    * Log-Loss\n",
    "    * Precison at k (P@K)\n",
    "\n",
    "  * Regression\n",
    "    * Mean Absoulte Error\n",
    "    * Mean Squared Error/ Roor MSE\n",
    "    * RMSLE\n",
    "    * MAPE : Mean Percentage Error\n",
    "    * R suqared\n",
    "\n",
    "#### Classification Metrics\n",
    "    ###### Confusion Matrix / Error Matrix\n",
    "    <div><img src=\"images/ConfusedMat.png\" width=\"1000\"/></div><br><br><br>\n",
    "\n",
    "  * FP, False Positive, False Alarm, type-I error: A test result which wrongly indicates a condition is **present** when it's not.\n",
    "  * FN, False Negative, Miss, type-II error: A test result which wrongly indicates a condition is **absent** when it's not.\n",
    "  * Prevalence: % of +ve class instances\n",
    "  * Accuracy: What % of instances are correctly classified?\n",
    "    * = (TP+TN)/(TP+FP+TN+FN) => `# of correctly classified instances` / `# total instances`\n",
    "\n",
    "  * Precision: What % of +ve predicted instances are actually +ve?\n",
    "     * = TP/(TP+FP) => `# of True Positives`/ `# of +ve predicted instances`\n",
    "\n",
    "  * Recall / Hit-Rate: What % of actual +ve instances detected?\n",
    "    * = TP/(TP+FN) => `# of True Positives`/ `# of actual +ve instances`\n",
    "    * Also Called, True Positive Rate (TPR), Sensitivity.\n",
    "\n",
    "  * Miss-rate: What % of actual +ve instances not detected?\n",
    "    * = FN/(TP+FN) => `# of +ve incorectly classified`/ `# of actual +ve instances`\n",
    "\n",
    "  * FPR / False Positive Rate / False Alarm Rate: What % of -ve class instances detected as +ve?\n",
    "    * = 1 - TNR\n",
    "\n",
    "  * True Negative Rate / Specificity: What % of actual -ve instances rejected (detected as -ve)?\n",
    "\n",
    "  * F1-Score: Simple weighted average (harmonic mean) of P and R; = 2*P*R / (P+R)\n",
    "\n",
    "Some terms which go hand by hand\n",
    "* Precision - Recall (TPR): For P-R curves\n",
    "* Sensitivity (TPR)- Specificity (TNR): For ROC curve, Sensitivity v/s 1-Specificity\n",
    "* TPR - FPR: ROC Curves\n",
    "* Hit Rate (TPR) - Miss Rate (FNR)\n",
    "\n",
    "##### P-R Curve v/s ROC curve\n",
    "**Why P-R or ROC curves are used in classification?**\n",
    "When a model predicts the probability of the +ve class i.e. logistic regression, it can be more flexible to assign the +ve class if the probability is above a threshold, <br>so we can play with some choices of threshold based on the problem where the cost of one error outweighs the cost of other type or errors i.e. type I and type II errors. P-R and ROC curve are the two diagnostic tools that help in the interpretation of probabilistic forecast for binary classification.\n",
    "\n",
    "**P-R Curve:**\n",
    "* Summarizes trade-off between Recall(TPR) and Precision for a predictive model.\n",
    "* Precision on x-axis and Recall on y-axis. When Precision increases, Recall decreases and vice versa.\n",
    "* Helps finding a threshold such that you will have a good Recall or good Precision depending on the problem given a sufficient Precision or Recall correspondingly.\n",
    "* Used for highly imbalanced classes.\n",
    "\n",
    "**ROC Curve / Receiver Operating Curve**\n",
    "* Summarizes trade-off between TPR and FPR for a predictive model.\n",
    "* Sensitivity/Recall/Hit-Rate/TPR on y-axis v/s 1-specificity/1-TNR/FPR on x-axis.\n",
    "* Helps finding a threshold such that you will have a good TPR given tolerable FPR depending upon the problem (cost).\n",
    "* Used for moderately im-balance classes.\n",
    "\n",
    "**AUC - Area Under Curve (ROC/PR) is another metric to assess the model.**\n",
    "  * 0 <= AUC < 0.5: There must be some logical or code bug, you might be assigning wrong class with the probability.\n",
    "  * AUC = 0.5: Similar to the random assigning of the class.\n",
    "  * 0.5 < AUC <= 1: better than a random model, if 1 then a perfect model.\n",
    "\n",
    "<div><img src=\"images/ROC.png\" width=\"300\"/> <img src=\"images/PR.png\" width=\"300\"/></div><br><br><br>\n",
    "\n",
    "#### Regression Metrics:\n",
    "* R-squared: quantifies how good your model explains the variability of the target variable\n",
    "    * = 1 - `variance explained by the model` / `variance of the taget variable`\n",
    "    * = 1 - `squared error of the model` / `squared error of the mean model`\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
