{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Author:** Vivek Singh Solanki\n",
    "**Book:** Approaching Almost Any Machine Learning Problem.\n",
    "**Date:**  04-03-2023\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning\n",
    "## Supervised v/s Unsupervised ML\n",
    "  * **Supervised ML:** Learning models where predicting single or multiple variables.\n",
    "    * <b>Classification v/s Regression:</b> Predicting a category v/s a numeric value.\n",
    "  * <b>Un-supervised ML:</b> No target variable, and we need to find patterns or group them.\n",
    "    * __Clustring:__ Instances with no target variable, can be grouped/divided into clusters and helps us to identity patterns. <br> i.e. Credit Card Transactions, where lots of data comes every second, and it gets very difficult to mark them fraud/genuine trx using humans. We can use clustering to group those transactions and find patterns and try to figure out abnormal behaviours."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross-Validation\n",
    "* It's a step in process of building ML model for a problem that ensures that the ML model *fits the data accurately* and make sure that it does not *overfit*.\n",
    "* Helps us choose best model from a set of candidate hypotheses.\n",
    "* Helps us find the estimate of performance of the model for production-live version, by mimicking the production live enviornment using the existing data.\n",
    "\n",
    "##### Overfitting:\n",
    "* When a model learns the training data well but fails to generalize unseen samples/ test data. High performance on train-set but very low on test-set.\n",
    "* A hypothesis overfits the training examples, if some other hypothesis that fits the training data less well by performs better on unseen data or over the entire distribution of instances.\n",
    "* High Variance, Low Bias.\n",
    "\n",
    "*Occam's Razor* stats that one should not try to complicate things that can be solved in much simpler manner.\n",
    "**Arises when;**\n",
    "   * We use a complex hypothesis/model for simpler ones.\n",
    "   * When there is noise in the data\n",
    "   * \\# of training examples is too small to produce a representative sample of the true target function.\n",
    "      *\n",
    "\n",
    "##### How Cross-Validation is done?\n",
    "It's simple divide data into two sets, one to train the models and another to test (hold-out/validation/test set) the performance. Though there many methods to cross-validate the models;\n",
    " * __Hold-out:__ above discussed. When data is large and it's expensive to train model several times. time-series data. etc\n",
    " * __k-fold:__ Divide the data randomly into k sets exclusive of each other, use k-1 sets for training and remained one set to test and repeat this k times.\n",
    " * __stratified k-fold:__ when classes are highly imbalanced, dividing randomly could lead no or very less samples for small class, you need to ensure the proportion of that in each of the k sets.\n",
    " * __Leave one out: __\n",
    " * __Group k-fold:__\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation Metrics\n",
    "  * Classifications Metrics:\n",
    "    * Accuracy, Precision, Recall, F1-Score, TPR, FPR, Area under the ROC curve\n",
    "    * Log-Loss\n",
    "    * Precison at k (P@K)\n",
    "\n",
    "  * Regression\n",
    "    * Mean Absoulte Error\n",
    "    * Mean Squared Error/ Roor MSE\n",
    "    * RMSLE\n",
    "    * MAPE : Mean Percentage Error\n",
    "    * R suqared\n",
    "\n",
    "#### Classification Metrics\n",
    "    ###### Confusion Matrix / Error Matrix\n",
    "    <div><img src=\"images/ConfusedMat.png\" width=\"1000\"/></div><br><br><br>\n",
    "\n",
    "  * FP, False Positive, False Alarm, type-I error: A test result which wrongly indicates a condition is **present** when it's not.\n",
    "  * FN, False Negative, Miss, type-II error: A test result which wrongly indicates a condition is **absent** when it's not.\n",
    "  * Prevalence: % of +ve class instances\n",
    "  * Accuracy: What % of instances are correctly classified?\n",
    "    * = (TP+TN)/(TP+FP+TN+FN) => `# of correctly classified instances` / `# total instances`\n",
    "\n",
    "  * Precision: What % of +ve predicted instances are actually +ve?\n",
    "     * = TP/(TP+FP) => `# of True Positives`/ `# of +ve predicted instances`\n",
    "\n",
    "  * Recall / Hit-Rate: What % of actual +ve instances detected?\n",
    "    * = TP/(TP+FN) => `# of True Positives`/ `# of actual +ve instances`\n",
    "    * Also Called, True Positive Rate (TPR), Sensitivity.\n",
    "\n",
    "  * Miss-rate: What % of actual +ve instances not detected?\n",
    "    * = FN/(TP+FN) => `# of +ve incorectly classified`/ `# of actual +ve instances`\n",
    "\n",
    "  * FPR / False Positive Rate / False Alarm Rate: What % of -ve class instances detected as +ve?\n",
    "    * = 1 - TNR\n",
    "\n",
    "  * True Negative Rate / Specificity: What % of actual -ve instances rejected (detected as -ve)?\n",
    "\n",
    "  * F1-Score: Simple weighted average (harmonic mean) of P and R; = 2*P*R / (P+R)\n",
    "\n",
    "Some terms which go hand by hand\n",
    "* Precision - Recall (TPR): For P-R curves\n",
    "* Sensitivity (TPR)- Specificity (TNR): For ROC curve, Sensitivity v/s 1-Specificity\n",
    "* TPR - FPR: ROC Curves\n",
    "* Hit Rate (TPR) - Miss Rate (FNR)\n",
    "\n",
    "##### P-R Curve v/s ROC curve\n",
    "**Why P-R or ROC curves are used in classification?**\n",
    "When a model predicts the probability of the +ve class i.e. logistic regression, it can be more flexible to assign the +ve class if the probability is above a threshold, <br>so we can play with some choices of threshold based on the problem where the cost of one error outweighs the cost of other type or errors i.e. type I and type II errors. P-R and ROC curve are the two diagnostic tools that help in the interpretation of probabilistic forecast for binary classification.\n",
    "\n",
    "**P-R Curve:**\n",
    "* Summarizes trade-off between Recall(TPR) and Precision for a predictive model.\n",
    "* Precision on x-axis and Recall on y-axis. When Precision increases, Recall decreases and vice versa.\n",
    "* Helps finding a threshold such that you will have a good Recall or good Precision depending on the problem given a sufficient Precision or Recall correspondingly.\n",
    "* Used for highly imbalanced classes.\n",
    "\n",
    "**ROC Curve / Receiver Operating Curve**\n",
    "* Summarizes trade-off between TPR and FPR for a predictive model.\n",
    "* Sensitivity/Recall/Hit-Rate/TPR on y-axis v/s 1-specificity/1-TNR/FPR on x-axis.\n",
    "* Helps finding a threshold such that you will have a good TPR given tolerable FPR depending upon the problem (cost).\n",
    "* Used for moderately im-balance classes.\n",
    "\n",
    "**AUC - Area Under Curve (ROC/PR) is another metric to assess the model.**\n",
    "  * 0 <= AUC < 0.5: There must be some logical or code bug, you might be assigning wrong class with the probability.\n",
    "  * AUC = 0.5: Similar to the random assigning of the class.\n",
    "  * 0.5 < AUC <= 1: better than a random model, if 1 then a perfect model.\n",
    "\n",
    "<div><img src=\"images/ROC.png\" width=\"300\"/> <img src=\"images/PR.png\" width=\"300\"/></div><br><br><br>\n",
    "\n",
    "#### Regression Metrics:\n",
    "* R-squared: quantifies how good your model explains the variability of the target variable\n",
    "    * = 1 - `variance explained by the model` / `variance of the taget variable`\n",
    "    * = 1 - `squared error of the model` / `squared error of the mean model`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Engineering\n",
    "It's not just about creating new features, but also includes different types of normalization/ transformations.\n",
    "\n",
    "#### Variable Types\n",
    "##### Categorical Variables\n",
    "* **Binary**: 2 Categories\n",
    "* **Nominal**: 2+ Categories\n",
    "* **Ordinal**: \"levels\" of categories with particular order associated. i.e. cold, warm, hot.\n",
    "* **Cyclic**: i.e. weekdays, month-days etc.\n",
    "\n",
    "###### Encoding Techniques for categorical variables\n",
    "* **Label Encoding:** Assigns a number to every category i.e. cold -> 0, warm-> 1, hot->2.\n",
    "  * This cannot be done for the linear or NN models as data expected to be standardized/scaled.\n",
    "* **Binary Encoding:** Assigns a binary reprsenation to every category where each digit is stored in a different column i.e. cold -> 0 | 0, warm -> 0 | 1, hot -> 1 | 1\n",
    "* **One hot Encoding:** It will create a dummy variable for each category, where it will contain 1 if the category present else 0. i.e. cold -> 1 | 0 | 0, warm -> 0 | 1 | 0, hot -> 0 | 0 | 1\n",
    "  * If there are many (10s of 1000s) categorical variables, that will create a huge # of dummy variables (a huge feature space), where a model can take forever to train.\n",
    "    * Solutions: \\*Below Solutions can be applied conditioned on if model is compatible with it.\n",
    "        * Use simpler model which can run fast i.e. logistic regressions vs boosting methods. (if possible)\n",
    "        * Use label encoding. (if possible)\n",
    "        * Compress the feature space using TruncatedSVD, PCA etc.\n",
    "        * Use spare matrices instead of dense ones.\n",
    "        * use target encoding.\n",
    "        * Use entity embeddings, i.e. in NLP, BoW -> vectors.\n",
    "* **Target Encodings**: Maps each category in your feature list to its mean/ median/ capped mean/ std/ n%tile value etc.\n",
    "  * This can overfit your data. So you need to create a k-folds in data (like in  CV), and for each fold, to encode the categories you will aggregate <br>the stats in remaining folds and add it to the current one. And you fit the training models on the same folds. For the unseen data, target encodings can be derived from the whole training set.\n",
    "  * Use smoothing or add noise to avoid overfitting.\n",
    "* **Entity Embeddings:** Like Binary/One hot encoding, we reprsent each category using a vector of binary values, here the idea is to reduce the length of vectors by making it of floats.\n",
    "  *  i.e. BoW -> Vectors\n",
    "\n",
    "**Rare Category / New Category / Unseen Category:**\n",
    "Some categories will only appear very rare in the data, and hen a category was present in the training data but it appeared when the model was live, since the model never have seen that category it will throw an error.\n",
    "Solutions:\n",
    "* You can define many of these un-frequent categories into a single category \"Other\", and if some unknown/unseen/new category comes in you can put it in \"Other\" and the model won't fail.\n",
    "* Create a prediction model for the unknown categories, if you have idea that it will appear in future, and try to predict a large category which is most similar to the unknown one.\n",
    "\n",
    "##### Examples for Feature Engineering:\n",
    "* Categorical Encodings\n",
    "* Scaling/ Standardization/ Transformations/ Binning of Numerical features\n",
    "  * Many models need features to be standardized to bring it to same scale, e.g. linear regression.\n",
    "  * Log-transformation is required when feature is highly skewed, have high variance.\n",
    "  * Binning numeric features to categorical/numeric features e.g. for decision trees, though they handle it by themselves, so not required.\n",
    "  * Creating polynomial features from the existing features, to explain non-linearity in the data.\n",
    "* Handling Missing values:\n",
    "  * For categorical create a new category \"UNKNOWN\" or use \"mode\" to impute or create a prediction model like (k-nn) to predict. sklearn.KNNImputer\n",
    "  * For numerical features, impute with mean, median etc. or create a prediction model e.g. training a regression model as target=\"column with missing values\", features ='remainging features\". May overfit, so need to use CV techniques.\n",
    "\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
